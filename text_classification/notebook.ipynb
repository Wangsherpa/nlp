{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting en_core_web_md==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.8)\n",
      "Requirement already satisfied: thinc==7.4.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.10.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.7)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.66.0)\n",
      "Requirement already satisfied: setuptools in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (68.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/nlp/.venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytorch-lightning==1.6.5 spacy==2.2.4\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Download the conversation dataset and parse it into a pytorch dataset\n",
    "- Create Trainer function to help with multi-epoch training\n",
    "- Model 1: Simple Word2Vec + MLP Model\n",
    "- Model 2: Sliding window trigram (Word2Vec)\n",
    "- Model 3: Embedding bag based model on Trigramm\n",
    "\n",
    "## Dataset Information\n",
    "We'll be using the Empathetic Dialogues dataset open-sourced by Facebook ([link](https://github.com/facebookresearch/EmpatheticDialogues))\n",
    "\n",
    "The columns we'll primarily focus on are:\n",
    "1. context: emotion we're trying to predict\n",
    "2. prompt + utterance: We'll combine these sentences and use them as input\n",
    "\n",
    "Let's download and explore the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import csv\n",
    "\n",
    "DIRECTORY_NAME = \"data\"\n",
    "TRAIN_FILE = \"data/empatheticdialogues/train.csv\"\n",
    "VALIDATION_FILE = \"data/empatheticdialogues/valid.csv\"\n",
    "TEST_FILE = \"data/empatheticdialogues/test.csv\"\n",
    "\n",
    "def download_dataset():\n",
    "    \"\"\"\n",
    "    Download the dataset. The tarball contains three files: train.csv, valid.csv and test.csv\n",
    "    \"\"\"\n",
    "\n",
    "    !wget 'https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz'\n",
    "    if not os.path.isdir(DIRECTORY_NAME):\n",
    "        os.makedirs(DIRECTORY_NAME)\n",
    "    tar = tarfile.open('empatheticdialogues.tar.gz')\n",
    "    tar.extractall(DIRECTORY_NAME)\n",
    "    tar.close()\n",
    "\n",
    "# download_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/empatheticdialogues/test.csv',\n",
       " 'data/empatheticdialogues/train.csv',\n",
       " 'data/empatheticdialogues/valid.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the downloaded files\n",
    "import glob\n",
    "glob.glob(f\"{DIRECTORY_NAME}/**/*.csv\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30693/496746084.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('data/empatheticdialogues/train.csv', sep='\\\\n', header=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>utterance_idx</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_idx</th>\n",
       "      <th>utterance</th>\n",
       "      <th>selfeval</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>1</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>2</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>3</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>4</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hit:0_conv:1</td>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0       conv_id utterance_idx      context  \\\n",
       "1  hit:0_conv:1             1  sentimental   \n",
       "2  hit:0_conv:1             2  sentimental   \n",
       "3  hit:0_conv:1             3  sentimental   \n",
       "4  hit:0_conv:1             4  sentimental   \n",
       "5  hit:0_conv:1             5  sentimental   \n",
       "\n",
       "0                                             prompt speaker_idx  \\\n",
       "1  I remember going to the fireworks with my best...           1   \n",
       "2  I remember going to the fireworks with my best...           0   \n",
       "3  I remember going to the fireworks with my best...           1   \n",
       "4  I remember going to the fireworks with my best...           0   \n",
       "5  I remember going to the fireworks with my best...           1   \n",
       "\n",
       "0                                          utterance     selfeval tags  \n",
       "1  I remember going to see the fireworks with my ...  5|5|5_2|2|5       \n",
       "2  Was this a friend you were in love with_comma_...  5|5|5_2|2|5       \n",
       "3                This was a best friend. I miss her.  5|5|5_2|2|5       \n",
       "4                                Where has she gone?  5|5|5_2|2|5       \n",
       "5                                 We no longer talk.  5|5|5_2|2|5       "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see few examples from the dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/empatheticdialogues/train.csv', sep='\\\\n', header=None)\n",
    "df = df[0].str.split(',', expand=True)\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:]\n",
    "df.columns = new_header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a label encoder which converts our text labels to integer ids or vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_integer = dict()\n",
    "integer_to_label = dict()\n",
    "\n",
    "for ix, label in enumerate(df[\"context\"].unique()):\n",
    "    label_to_integer[label] = ix\n",
    "    integer_to_label[ix] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(file_path, num_samples=5000):\n",
    "    # read each row as a single column row\n",
    "    df = pd.read_csv(file_path, sep=\"\\\\n\", header=None)\n",
    "    # split up each row into separate columns\n",
    "    df = df[0].str.split(',', expand=True)\n",
    "    # set the header by using the first row\n",
    "    new_header = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df.columns = new_header\n",
    "\n",
    "    # convert labels to integers\n",
    "    df[\"target\"] = df[\"context\"].apply(lambda x: label_to_integer[x])\n",
    "    df[\"feature\"] = df[\"prompt\"] + \" \" + df[\"utterance\"]\n",
    "\n",
    "    # return df with only required columns: feature and target\n",
    "    return df[[\"target\", \"feature\"]].sample(n = num_samples, random_state=0).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will limit the sample size for train, valid and test set to speed up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30693/1362603666.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_path, sep=\"\\\\n\", header=None)\n",
      "/tmp/ipykernel_30693/1362603666.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_path, sep=\"\\\\n\", header=None)\n",
      "/tmp/ipykernel_30693/1362603666.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_path, sep=\"\\\\n\", header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (40000, 2)\n",
      "Shape of validation data: (4000, 2)\n",
      "Shape of test data: (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "training_data = parse_dataset(TRAIN_FILE, num_samples=40000)\n",
    "validation_data = parse_dataset(VALIDATION_FILE, num_samples=4000)\n",
    "test_data = parse_dataset(TEST_FILE, num_samples=4000)\n",
    "\n",
    "print(\"Shape of training data:\", training_data.shape)\n",
    "print(\"Shape of validation data:\", validation_data.shape)\n",
    "print(\"Shape of test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pytorch Dataset and Data loaders\n",
    "\n",
    "- **Dataset**: Dataset stores the samples and thier corresponding values.\n",
    "- **DataLoader**: Dataloader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "- **LightningDataModule**: A datamodule is a shareable, reusable class that encapsulates all the steps needed to process data. A datamodule encapsulates the five steps involved in data processing in PyTorch:\n",
    "    1. Download / tokenize / process.\n",
    "    2. Clean and (maybe) save to disk.\n",
    "    3. Load inside Dataset.\n",
    "    4. Apply transforms (rotate, tokenize, etc...)\n",
    "    5. Wrap inside a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    \"\"\"Creates an pytorch dataset to consume our pre-loaded csv data\"\"\"\n",
    "    def __init__(self, data, vectorizer):\n",
    "        self.dataset = data\n",
    "        # vectorizer needs to implement a vectorize function that returns vector and tokens\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        (label, sentence) =self.dataset[idx]\n",
    "        sentence_vector, sentence_tokens = self.vectorizer.vectorize(sentence)\n",
    "        return {\n",
    "            \"vectors\": sentence_vector,\n",
    "            \"label\": label,\n",
    "            \"tokens\": sentence_tokens, # for debugging only\n",
    "            \"sentence\": sentence # for debugging only\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataModule(pl.LightningDataModule):\n",
    "    \"\"\"LightningDataModule: Wrapper class for the dataset to be used in training\"\"\"\n",
    "    def __init__(self, vectorizer, params):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.train_data = ClassificationDataset(training_data, vectorizer)\n",
    "        self.validation_data = ClassificationDataset(validation_data, vectorizer)\n",
    "        self.test_data = ClassificationDataset(test_data, vectorizer)\n",
    "\n",
    "    # Function to convert the input raw data from the dataset into the model input.\n",
    "    def collate_fn(self, batch):\n",
    "        # Embedding layers need the inputs to be integer so we need to add this special case here.\n",
    "        if self.params.integer_input:\n",
    "            word_vector = [torch.LongTensor(item[\"vectors\"]) for item in batch]\n",
    "            sentence_vector = pad_sequence(word_vector, batch_first=True, padding_value=0)\n",
    "        else:\n",
    "            sentence_vector = torch.stack([torch.Tensor(item[\"vectors\"]) for item in batch])\n",
    "            # print(\"Batch type\",type(batch))\n",
    "        labels = torch.LongTensor([item[\"label\"] for item in batch])\n",
    "        return {\"vectors\": sentence_vector, \"labels\": labels, \"sentences\": [item[\"sentence\"] for item in batch]}\n",
    "    \n",
    "    # Training dataloader: will reset itself each epoch\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.params.batch_size, collate_fn=self.collate_fn, num_workers=4)\n",
    "    \n",
    "    # Validation dataloader: will reset itself each epoch\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validation_data, batch_size=self.params.batch_size, collate_fn=self.collate_fn, num_workers=4)\n",
    "    \n",
    "    # Test dataloader: will reset itself each epoch\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.params.batch_size, collate_fn=self.collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created the DataLoader and Datasets. Let's write the training and testing loops.\n",
    "`LightningModule` organizes the PyTorch code into 5 sections\n",
    "\n",
    "1. Computations (init).\n",
    "2. Train loop (training_step)\n",
    "3. Validation loop (validation_step)\n",
    "4. Test loop (test_step)\n",
    "5. Optimizers (configure_optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(pl.LightningModule):\n",
    "    def __init__(self, model, params):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=params.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"vectors\"]\n",
    "        y = batch[\"labels\"]\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, reduction=\"mean\")\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss},\n",
    "            batch_size=self.params.batch_size,\n",
    "            prog_bar=True\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x = batch[\"vectors\"]\n",
    "        y = batch[\"labels\"]\n",
    "        y_hat = self(x)\n",
    "        val_loss = F.cross_entropy(y_hat, y, reduction=\"mean\")\n",
    "        predictions = torch.argmax(y_hat, dim=1)\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": val_loss, \"val_acc\": self.accuracy(predictions, y)},\n",
    "            batch_size=self.params.batch_size,\n",
    "            prog_bar=True\n",
    "        )\n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self,batch, batch_nb):\n",
    "        x = batch[\"vectors\"]\n",
    "        y = batch[\"labels\"]\n",
    "        y_hat = self(x)\n",
    "        test_loss = F.cross_entropy(y_hat, y, reduction=\"mean\")\n",
    "        predictions = torch.argmax(y_hat, dim=1)\n",
    "        self.log_dict(\n",
    "            {\"test_loss\": test_loss, \"test_acc\": self.accuracy(predictions, y)},\n",
    "            batch_size=self.params.batch_size,\n",
    "            prog_bar=True\n",
    "        )\n",
    "        return test_loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        y_hat = self.model(batch[\"vectors\"])\n",
    "        predictions = torch.argmax(y_hat, dim=1)\n",
    "        return {\"logits\": y_hat, \"predictions\": predictions,\n",
    "                \"labels\": batch[\"labels\"], \"sentences\": batch[\"sentences\"]}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.params.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a Lightning and LightningDataModule, a `Trainer` automates everything else.\n",
    "Let's write a helper function that takes the model, vectorizer, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, params, vectorizer):\n",
    "    # Create a pytorch trainer\n",
    "    trainer = pl.Trainer(max_epochs=params.max_epochs, check_val_every_n_epoch=1)\n",
    "\n",
    "    # Initialize our data loader with the passed vectorizer\n",
    "    data_module = ClassificationDataModule(vectorizer, params)\n",
    "\n",
    "    # Instantiate a new model\n",
    "    model = EmotionClassifier(model, params)\n",
    "\n",
    "    # Train and validate the model\n",
    "    trainer.fit(model, data_module.train_dataloader(), val_dataloaders=data_module.val_dataloader())\n",
    "\n",
    "    # Test the model\n",
    "    trainer.test(model, data_module.test_dataloader())\n",
    "\n",
    "    # Predict on the same test set to show some output\n",
    "    output = trainer.predict(model, data_module.test_dataloader())\n",
    "\n",
    "    for i in range(2):\n",
    "        print(\"#########\")\n",
    "        print(f\"Sentence: {output[1]['sentences'][i]}\")\n",
    "        print(f\"Predicted Emotion: {integer_to_label[output[1]['predictions'][i].item()]}\")\n",
    "        print(f\"Actual Label: {integer_to_label[output[1]['labels'][i].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Model 1: Average word vector of the sentence - Baseline\n",
    "\n",
    "Let's build the first simple word2vec based model for the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import en_core_web_md\n",
    "\n",
    "# load the entire space word-vector index in memory\n",
    "loaded_spacy_model = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorClassifier(torch.nn.Module):\n",
    "    def __init__(self, word_vec_dimension, num_classes):\n",
    "        super().__init__()\n",
    "        self.classes = num_classes\n",
    "        self.linear_layer = torch.nn.Linear(word_vec_dimension, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"Projection from word_vec_dim to n_classes\n",
    "        \n",
    "        Batch is the shape (batch_size, max_seq_len, word_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.linear_layer(batch)\n",
    "\n",
    "class HParams:\n",
    "    batch_size: int = 32\n",
    "    integer_input: bool = False\n",
    "    word_vec_dimension: int = 300\n",
    "    num_classes: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    max_epochs: int = 10\n",
    "\n",
    "\n",
    "class SpacyVectorizer:\n",
    "    def vectorize(self, sentence):\n",
    "        \"\"\"Given a sentence, tokenize it and reference pre-trained word vector for each token.\n",
    "        \n",
    "        Returns a tuple of sentence_vector and list of text tokens\n",
    "        \"\"\"\n",
    "        sentence_vector = []\n",
    "        sentence_tokens = []\n",
    "        spacy_doc = loaded_spacy_model.make_doc(sentence) # I am Wang\n",
    "        word_vector = [token.vector for token in spacy_doc] ## [ [Embedding of I], [Embedding of am], [Embedding of UNK] ]\n",
    "        sentence_tokens = list([token.text for token in spacy_doc])\n",
    "        sentence_vector = np.mean(np.array(word_vector), axis=0)\n",
    "        return sentence_vector, sentence_tokens\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /workspaces/nlp/text_classification/lightning_logs\n",
      "\n",
      "  | Name     | Type                 | Params\n",
      "--------------------------------------------------\n",
      "0 | model    | WordVectorClassifier | 9.6 K \n",
      "1 | accuracy | MulticlassAccuracy   | 0     \n",
      "--------------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1375/1375 [00:12<00:00, 105.89it/s, loss=2.05, v_num=0, train_loss=1.930, val_loss=2.240, val_acc=0.371]\n",
      "Testing DataLoader 0: 100%|██████████| 125/125 [00:01<00:00, 107.17it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.36250001192092896\n",
      "        test_loss            2.229304552078247\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 125/125 [00:01<00:00, -1066.76it/s]\n",
      "#########\n",
      "Sentence: When I got falsely accused of eating my roommate's ice cream last night_comma_ I was completely outraged! I don't even eat ice cream because I'm lactose intolerant_comma_ and she knows that! I wonder who actually ate it then.\n",
      "Predicted Emotion: guilty\n",
      "Actual Label: furious\n",
      "#########\n",
      "Sentence: I found out that my childhood cat passed away yesterday  Thats sad. I love cats\n",
      "Predicted Emotion: sad\n",
      "Actual Label: sad\n"
     ]
    }
   ],
   "source": [
    "trainer(model=WordVectorClassifier(HParams.word_vec_dimension, HParams.num_classes),\n",
    "        params=HParams,\n",
    "        vectorizer=SpacyVectorizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "    batch_size: int = 32\n",
    "    integer_input: bool = False\n",
    "    word_vec_dimension: int = 300\n",
    "    num_classes: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    max_epochs: int = 10\n",
    "    n_grams: int = 3\n",
    "\n",
    "\n",
    "class SpacyChunkVectorizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "    def vectorize(self, sentence):\n",
    "        \"\"\"Given a sentence, tokenize it and reference pre-trained word vector for each token.\n",
    "        \n",
    "        Returns a tuple of sentence_vector and list of text tokens\n",
    "        \"\"\"\n",
    "        sentence_vector = []\n",
    "        sentence_tokens = []\n",
    "        spacy_doc = loaded_spacy_model.make_doc(sentence) # I am Wang\n",
    "        word_vector = [token.vector for token in spacy_doc] ## [ [Embedding of I], [Embedding of am], [Embedding of UNK] ]\n",
    "        sentence_tokens = list([token.text for token in spacy_doc])\n",
    "        i = 0\n",
    "        trigrams = []\n",
    "        flag = False\n",
    "        while i+3 < len(word_vector)+1:\n",
    "            flag = True\n",
    "            trigrams.append(np.hstack(word_vector[i:i+self.params.n_grams]))\n",
    "            i += 1\n",
    "\n",
    "        if not flag:\n",
    "            # print(\"True\")\n",
    "            temp_lst = []\n",
    "            for w in word_vector:\n",
    "                temp_lst.append(w)\n",
    "            while len(temp_lst) < self.params.n_grams:\n",
    "                temp_lst.append(word_vector[-1])\n",
    "            trigrams.append(np.hstack(temp_lst))\n",
    "        \n",
    "        if len(trigrams) == 0:\n",
    "            raise Exception(f\"Empty trigrams, {len(word_vector)} {sentence}\")\n",
    "        sentence_vector = np.mean(np.array(trigrams), axis=0)\n",
    "        # print(\"###\", sentence_vector.shape)\n",
    "        return sentence_vector, sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = SpacyChunkVectorizer(HParams)\n",
    "vector, _ = v.vectorize(\"My name is Wang\")\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type                 | Params\n",
      "--------------------------------------------------\n",
      "0 | model    | WordVectorClassifier | 28.8 K\n",
      "1 | accuracy | MulticlassAccuracy   | 0     \n",
      "--------------------------------------------------\n",
      "28.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "28.8 K    Total params\n",
      "0.115     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1375/1375 [00:16<00:00, 83.88it/s, loss=1.83, v_num=1, train_loss=1.710, val_loss=2.110, val_acc=0.394]\n",
      "Testing DataLoader 0: 100%|██████████| 125/125 [00:01<00:00, 89.60it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.3932499885559082\n",
      "        test_loss           2.1019606590270996\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 125/125 [00:01<00:00, -783.54it/s] \n",
      "#########\n",
      "Sentence: When I got falsely accused of eating my roommate's ice cream last night_comma_ I was completely outraged! I don't even eat ice cream because I'm lactose intolerant_comma_ and she knows that! I wonder who actually ate it then.\n",
      "Predicted Emotion: ashamed\n",
      "Actual Label: furious\n",
      "#########\n",
      "Sentence: I found out that my childhood cat passed away yesterday  Thats sad. I love cats\n",
      "Predicted Emotion: devastated\n",
      "Actual Label: sad\n"
     ]
    }
   ],
   "source": [
    "trainer(\n",
    "    model=WordVectorClassifier(\n",
    "        HParams.word_vec_dimension * HParams.n_grams,\n",
    "        HParams.num_classes\n",
    "    ),\n",
    "    params=HParams,\n",
    "    vectorizer=SpacyChunkVectorizer(HParams)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 'My', 'My ', 'y n', ' na', 'nam', 'ame']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My name\"\n",
    "char_trigrams = [*ngrams(text, 3, pad_left=True, left_pad_symbol=\"\")]\n",
    "char_trigrams_token = [''.join(chars) for chars in char_trigrams]\n",
    "char_trigrams_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: EmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParamsCTT:\n",
    "    batch_size: int = 16\n",
    "    integer_input: bool = True\n",
    "    num_classes: int = 32\n",
    "    learning_rate: float = 0.001\n",
    "    max_epochs: int = 10\n",
    "    n_grams: int = 3\n",
    "    embed_dim: int = 350\n",
    "    num_tokens: int = 5000\n",
    "\n",
    "\n",
    "class CharacterTrigramTokenizer:\n",
    "    \"\"\"\n",
    "    We represent a sentence as vector of num_tokens tokens.\n",
    "    If the trigram is present in the sentence then we add the token's id to the sentence.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data, num_tokens):\n",
    "        self.num_tokens = num_tokens\n",
    "        self.token_to_id_map = self.get_char_trigram_token_map(train_data, num_tokens)\n",
    "\n",
    "    def get_char_trigrams(self, sentence):\n",
    "        char_trigrams = [*ngrams(sentence, 3, pad_left=True, left_pad_symbol=\"\")]\n",
    "        char_trigrams_token = [''.join(chars) for chars in char_trigrams]\n",
    "        return char_trigrams_token\n",
    "\n",
    "    def get_char_trigram_token_map(self, train_data, num_tokens):\n",
    "        count = Counter()\n",
    "        for label, sentence in train_data:\n",
    "            char_trigrams_token = self.get_char_trigrams(sentence)\n",
    "            count.update(char_trigrams_token) \n",
    "\n",
    "        token_to_id_map = {d[0]: i+1 for i, d in enumerate(count.most_common(num_tokens))}\n",
    "        return token_to_id_map\n",
    "    \n",
    "    def vectorize(self, sentence):\n",
    "        trigrams = self.get_char_trigrams(sentence)\n",
    "        sentence_vector = [self.token_to_id_map[trigram] if trigram in self.token_to_id_map else 0 for trigram in trigrams]\n",
    "        return sentence_vector, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " th 1\n",
      " I  2\n",
      "the 3\n",
      " to 4\n",
      "ing 5\n"
     ]
    }
   ],
   "source": [
    "ctt = CharacterTrigramTokenizer(training_data, HParamsCTT.num_tokens)\n",
    "i = 0\n",
    "for k, v in ctt.token_to_id_map.items():\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(k, v)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the simple embedding layer based model and start training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, num_tokens, embed_dim, n_classes):\n",
    "        super().__init__()\n",
    "        self.classes = n_classes\n",
    "        self.embedding = torch.nn.EmbeddingBag(num_tokens, embed_dim)\n",
    "        self.linear = torch.nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        embed = self.embedding(batch)\n",
    "        return self.linear(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type                            | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model    | EmbeddingBagClassificationModel | 1.8 M \n",
      "1 | accuracy | MulticlassAccuracy              | 0     \n",
      "-------------------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2750 [00:00<?, ?it/s]                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2750/2750 [00:27<00:00, 100.85it/s, loss=1.02, v_num=2, train_loss=0.625, val_loss=2.040, val_acc=0.438]\n",
      "Testing DataLoader 0: 100%|██████████| 250/250 [00:00<00:00, 281.76it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.40299999713897705\n",
      "        test_loss           2.1087276935577393\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Predicting DataLoader 0: 100%|██████████| 250/250 [00:00<00:00, -3130.87it/s] \n",
      "#########\n",
      "Sentence: I am very hopeful to go on vacation this summer Summer can be over in July for teachers sometimes. Are you a teacher?\n",
      "Predicted Emotion: hopeful\n",
      "Actual Label: hopeful\n",
      "#########\n",
      "Sentence: I just built a warm fire and huddled up next to it until i started to fall asleep. I felt safe and secure I love a warm fire outside while camping! Sounds like a great time.\n",
      "Predicted Emotion: excited\n",
      "Actual Label: content\n"
     ]
    }
   ],
   "source": [
    "trainer(\n",
    "    model = EmbeddingBagClassificationModel(\n",
    "        HParamsCTT.num_tokens + 1,\n",
    "        HParamsCTT.embed_dim,\n",
    "        HParamsCTT.num_classes\n",
    "    ),\n",
    "    params=HParamsCTT,\n",
    "    vectorizer=ctt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b32df7775a17e109\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b32df7775a17e109\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
